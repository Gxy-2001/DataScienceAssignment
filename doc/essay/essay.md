# COVID-19背景下基于微博文本的网络心态分析

## 	1.  摘要

​		微博作为一种快速便捷的信息传播载体，在疫情阶段发挥着重要的信息传输作用。本次作业的目的就是深入分析疫情信息中蕴含的网民情绪及其变化情况。以新冠肺炎疫情相关的短微博和相关新闻下的评论作为主要研究对象，利用心态词典方法可以大致观察心态演变，使用拓展的多维心态词典分析不同阶段主要事件下的网民心态极性，并结合层次聚类法从中抽取网民关注热点，最后通过可视化方法展现相应的结果。结果TODO

​		**关键词**：微博；疫情；情感分析；心态词典；层次聚类；数据可视化

## 	2. 前言

### 2.1.研究背景

疫情：新冠肺炎病毒于2020年1月大规模爆发，虽然现代医疗科技发展迅速，但人类依然无法避免类似的大规模突发性公共卫生事件的困扰，可能在未来的很长一段时间内，大型传染性疾病依然无法彻底消除，人类将与病毒进行长期的抗争。突发卫生事件的突然性和危害性、新冠病毒的感染数和致死率等严重影响公众群体的正常心态，一种疫情下独特的网络社会心态就此形成，因此我们希望借助一些数据计量方法准确客观的了解公众群体的网络社会心态及其变化规律，并对未来的突发卫生事件产生积极意义。

微博：自疫情爆发以来，以微博为代表的社交媒体上广泛的传播各种疫情信息，用户发布的疫情相关短文本可以使用官方搜索引擎获取；此外，代表着官方媒体的央视新闻和人民日报等账号号通过微博发布了大量的疫情信息，同时微博的评论功能使得网友可以发表自己对于新闻的看法，这为我们数据分析提供了必要的数据源支持。

情感分析：情感分析是对带有情感色彩的主观性文本进行分析和推理的过程，其主要分析方法有机器学习方法和心态词典方法，本次作业主要使用心态词典及其相关联信息分析情感。

心态词典：心态词典法是最简单也是最符合直觉的方法，其核心是通过判断特定的关键字是否出现在文本中从而给文本确定情感倾向性。基于情感词典法主要将情感词表和人工制定的相关规则结合。这里面最主要的一个问题就是无法解决未录入词表词的问题，尤其是很多包含情感倾向性的新兴网络词汇，导致最后结果的召回率偏低。本次作业将重点考虑这部分的词语，以求得更高的准确率。

基于情感字典的具体做法是，用己有的人工标注的情感词典去査找一个文本中包含正向情感词汇和负向情感词汇的总分数，根据以下公式计算情感分数并判断情感极性和情感强烈程度。

<img src="essay.assets/图片3.png" alt="图片3" style="zoom:67%;" />

此公式中的Score代表情感分数，x<sub>i</sub>表示每个词语对应在情感词典里的分数值，而E表示对于分数值的权重处理，这部分将在下面有详细的介绍。而利用Score的正负情况可以得到情感的极性，Score的数值大小可以得到情感的强烈程度。

<img src="essay.assets\情感极性.png" alt="情感极性" style="zoom:67%;" />

关注热点：最能体现一段文本的热点信息就是关键词，本文实现了比较常用的textRank算法，能较好的完成提取关键信息的任务，此外，聚类是另一个提取热点的方法，它是根据目标文本的特征对其进行分类的方法，其结果能减少减少对象的类数，使得我们能够快速获取想要的文本热点。具体的聚类方法包括K-means算法，DBSCAN密度聚类和层次聚类等。

### 2.2. 研究~~

## 3. 研究方法

我们研究工作的目的是希望得到疫情期间网络公众群体的心态，具体的研究可以细化为两个部分，一是分析疫情期间不同时间段的用户情感极性和情感强烈程度，二是分析突发公共卫生事件下不同阶段公众信息的主题特征。

本作业的主要工作首先就是从网上抓取能代表网民心态的信息，然后要对这些信息进行初步的分类和预处理，之后使用三种技术对数据进行分析。我们使用心态词典研究情感极性和情感强度，使用拓展过的多维情感词典对于文本进行多维情感分类，并利用聚类算法分析主题特征，最后利用可视化绘图技术总结分析得到结论。详细的分析见下面的内容，主要涵盖的内容如下图。

<img src="essay.assets/流程.png" alt="流程" style="zoom:80%;" />



### 3.1. 数据获取

#### 3.1.1. 数据源选择

本次作业希望得到能代表网民情感的数据，最初我想到了QQ空间和微信朋友圈的内容，但从可行性的角度上这较难实现；其次可以是抖音、b站相关疫情视频上的弹幕或评论，但随着我们深入了解，发现这部分的数据收视频内容影响严重，许多弹幕存在重复的现象，并不利于我们的进一步分析；我们还考虑过京东、淘宝等电商应用内相关防疫物品(口罩，消毒工具和食品)下的评价和购买量，但经过调研我们发现疫情期间在售的商品普遍都已经下架，现在在架的商品评价均在2020年4月以后，这也不符合我们的需求。

所以本次数据采用新浪微博（简称微博）里的数据，微博是一种被大众广泛使用的社交平台，其信息传播快，更新速度快，信息类型远多于其他平台。在微博上，网民不仅可以接收信息，还可以可以自己发布自己的文本，也可以到其他人的微博下进行评论或转发，甚至很多情况下有人使用微博来传递重要信息或者处理应急事件，中国社会科学院社会学研究所在2020年发布的一项调查报告显示，75%以上的中国网民曾使用过微博来了解疫情相关信息，所以我们选择微博进行分析。

#### 3.1.2. 数据爬取

具体而言，我们需要的数据是2020年1月20日到2020年5月的疫情相关短微博和同时间下某些重点微博下的相关评论，总之就是单条微博的数据和微博下的评论数据。微博的网页端weibo.cn是几个微博接口中最好的选择，提供了我们需要的搜索功能，我就可以借助它来查询特定时间和特定关键词的微博文本。爬虫的主要内容是先在浏览器中请求微博，然后对于得到的内容进行解析。

##### 3.1.2.1. 请求

请求部分使用`requests`库，目标是微博网页端搜索的`url`，为了能够保证在请求时不用登陆，可以在请求的同时传入`Cookie`，因为要搜索特定时间和特定关键词的微博，所以在请求时还要传入关键词和时间，排序方法要按照时间排序。传入的关键词是疫情相关的随机关键词，我找到了约50个疫情相关的词语，包括积极词，中性词和消极词，并利用`random`模块随机生成数字下标，进行随机抽取关键词搜索，以求得到的数据能代表当天的网民主流心态。部分关键代码解释如下：

```python
import requests
url = 'https://weibo.cn/search/mblog' # 网页端微博搜索url
headers = {
    'Cookie': '自己的Cookie',
    'User-Agent': UserAgent().random  # fake_useragent库
}
params = {
    'keyword': self.keyword,          # 随机关键词
    'endtime': self.end_time,		  # 传入搜索时间，精确到小时，所以爬虫以小时为单位
    'sort': 'time',					  # 依据时间对微博进行排序
}
res = requests.get(url=url, params=params, headers=headers) # 仅展示部分代码
```

##### 3.1.2.2. 解析

解析部分使用lxml库，详见具体代码解读。

```python
from lxml import etree
# 使用etree对html进行解析
html = etree.HTML(res.text.encode('utf-8'))
# 在请求时page从1开始，weibos即每页的微博列表
# 等到len(weibos)==0就说明微博为空，此时停止即可
# 我们也不需要太多的文本，每个时间单位爬取大约十页内容即可
weibos = html.xpath("//div[@class='c' and @id]")
```

下面来解析`weibos`中的每个元素中微博id和微博发布者

``` python
# 对于该微博id的获取
# 利用xpath抽取id属性，体现为@id，后9位是该微博的id号
weibo['id'] = info.xpath('@id')[0]  # M_ItwJTv2Xg
weibo['id'] = weibo['id'][2:]		# ItwJTv2Xg
# 对该微博发布者进行爬取
user_info = html.xpath('//div[@class="ut"]/span[@class="ctt"]')[0]
# 例如'男神优衣裤 男/广东 加关注'
# '男神优衣裤' '男' '广东' '加关注' 分别是姓名，性别，省份，简介
# 发布者关注：关注[190]
following = html.xpath('//div[@class="tip2"]/a[1]/text()')[0]
# 发布者粉丝：粉丝[28031]
followd = html.xpath('//div[@class="tip2"]/a[2]/text()')[0]
```

解析正文

```python
# 把所有标签中的文本(包括嵌套)提取出来,并且放到一个字符串中
weibo_content = html.xpath('string(.)')
# 例如：'无悔无花:我刚刚关注了肺炎患者求助超话，抗击肺炎疫情，我们在一起，共同关注→肺炎患者求助  赞[0] 转发[0] 评论[0] 收藏 2020-02-10 10:59:58 来自超话'
# 然后截取所有 '赞' 之前的内容就是正文
weibo_content = weibo_content[:weibo_content.rfind(u'赞')]
# 包括发布微博的具体时间和来自设备都基于此
# 微博的点赞，转发和评论数自然也是如此，就不详述了，可见代码
```

解析完所有的数据后，要把每一条微博写入csv文件

``` python
result_headers = ['微博id', '发布者昵称', '发布者性别', '发布者地区', '发布者关注数', '发布者粉丝数', '微博正文', '发布时间', '点赞数', '转发数','评论数', ]
result_data = [w.values() for w in self.weibo][wrote_num:]
with open('topic/' + 'some keyWords' + str(mystart_day) + '.csv', 'a', encoding='utf-8-sig',
          newline='') as f:
    writer = csv.writer(f)
    global boo
    if boo:
        # 第一次要写表头，后序不用写了
        boo = False
        writer.writerows([result_headers])
        writer.writerows(result_data)
```

##### 3.1.2.3.  防反爬虫

由于微博平台有着严格的反爬虫策略，经常会被封号，所以我也采取了一些措施来防反爬虫。首先是自己设置了`user-agent`，利用`from fake_useragent`中的`UserAgent().random`来生成随机的`user-agent`，且每次爬虫完毕都会暂停随机几毫秒，来防止速度过快:`sleep(random.randint(6, 10))`，并且我使用了ip全局代理来通过不同ip进行访问，防止ip被封。

##### 3.1.2.4. 爬虫结果

我爬取了疫情期间2020年1月23日至2020年5月21日共计50MB数据，这些数据基于随机抽取的疫情关键词，且微博发布时间在每天都是随机且均匀的，然而可以看到微博中依然有很多并非网民发布的微博，这部分需要在后序进行筛选。数据总量在8-10万条。此外我还爬取了某些重点新闻下的评论，这几部分都作为我后序数据分析的数据源



### 2. 数据预处理

#### 3.2.1. 数据清洗

首先来看段示例文本。

```python
示例：'回复@让我再看你一眼 //让我再看你一眼·:#高以翔[超话]# 以翔 这个点了睡意全无  节日现在对于我们就是一种悲伤  没有了你世界都乱了 新冠肺炎疫情爆发让多少家庭除夕夜不能团聚  如果你在肯定会教我们正确的心态对待 会为逆行英雄打call 会做慈善捐助  因为你是Godfrey@高以翔Godfrey @全世界最好的高以翔Godfrey 仙桃  显示地图 原图http://t.cn/Ir1YpaAb0'
```

发现微博正文中有很多特殊字符，而且有很多格式是我们不需要的。例如微博中的@，这个表示微博和某人相关来给其一个提示，但是@后跟着的是用户昵称，这与文本内容无关，所以我们需要将其清洗掉。另外微博中含有很多表情符，例如`[微笑]`，感觉现代表情的含义是非常之丰富和复杂的，但绝大多数表情是为文本内容服务的，起到一个加强文本预期的作用，所以我可以将其处理掉，来减少后序任务的复杂性。剩余的例如网址、特殊字等请看下面的代码详解。

```python
# 清除用户名(:前的用户名)
for i in range(len(text)):
    if text[i] == ':' or text[i] == '：':
        text = text[i + 1:-1]
        break
        
# 清除网址url
zh_puncts1 = "，；、。！？（）《》【】"
URL_REGEX = re.compile(
            r'(?i)((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>' + zh_puncts1 + ']+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?«»“”‘’' + zh_puncts1 + ']))',
            re.IGNORECASE)
text = re.sub(URL_REGEX, "", text)

# 清除@和回复/转发中的用户名
text = re.sub(r"(回复)?(//)?\s*@\S*?\s*(:|：| |$)", " ", text) 

# 清除中括号包着的表情符号
text = re.sub(r"\[\S+?\]", "", text)

# 清除话题内容
text = re.sub(r"#\S+#", "", text)  

# 清除多个空格
text = re.sub(r"(\s)+", r"\1", text)

# 清除常见停用词标中的词，例如：展开、全文、转发、显示原图、原图 等等
for x in stop_terms:
    text = text.replace(x, "")
   
# 清除前后空格
text = text.strip()
```

对上述示例进行应用，可以得到以下结果。

```python
原文:'回复@让我再看你一眼·//让我再看你一眼·:#高以翔[超话]# 以翔 这个点了睡意全无  节日现在对于我们就是一种悲伤  没有了你世界都乱了 新冠肺炎疫情爆发让多少家庭除夕夜不能团聚  如果你在肯定会教我们正确的心态对待 会为逆行英雄打call 会做慈善捐助  因为你是Godfrey@高以翔Godfrey @全世界最好的高以翔Godfrey 仙桃 显示地图 原图 http://t.cn/Ir1YpaAb0'

清洗结果：'以翔 这个点了睡意全无 节日现在对于我们就是一种悲伤 没有了你世界都乱了 新冠肺炎疫情爆发让多少家庭除夕夜不能团聚 如果你在肯定会教我们正确的心态对待 会为逆行英雄打call 会做慈善捐助 因为你是Godfrey 仙桃'
```

#### 3.2.2. 去停用词和分词

停用词表我使用的是一份开源的中文停用词表，分词使用`jieba`分词的精确模式

```python
jieba.cut(line, cut_all=False, HMM=True)
```

#### 3.2.3. 提取关键词



### 3. 数据分析

#### 3.3.1.



### 4. 数据可视化

## 4. 案例

## 5. 结论

## 6. 反思与不足

## 7. 参考文献

## 8. 附录